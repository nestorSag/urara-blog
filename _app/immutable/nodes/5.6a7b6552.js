import{s as wt,i as Fe,j as ut,n as yt}from"../chunks/scheduler.9b9e513e.js";import{S as xt,i as bt,r as X,u as ee,v as te,d as ae,t as ne,w as se,g as s,s as o,h as i,A as r,c as l,j as Ge,f as t,k as N,a as n,y as V}from"../chunks/index.781c9930.js";import{P as _t,g as Ct,a as gt}from"../chunks/post_layout.65b0bc7d.js";import{I as Se}from"../chunks/footer.ec2e7159.js";function Tt(W){let h,u='<em>You can find <a href="https://github.com/nestorSag/textfont-ai" rel="nofollow noopener noreferrer external" target="_blank">this project’s repository on Github</a>, along with pretrained models and an interactive Dash web app.</em>',m,f,d='<a href="#motivation">Motivation</a>',p,c,Oe='At some point last year I was watching one of the episodes of <a href="https://en.wikipedia.org/wiki/Abstract%3A_The_Art_of_Design" rel="nofollow noopener noreferrer external" target="_blank">Abstract: the art of design</a> on Netflix, when I realised typeface generation was possibly a low hanging fruit from a machine learning perspective. Deep generative models have been shown to achieve impressive results in a variety of tasks, particularly image generation, and this includes datasets such as faces, landscapes and even painting styles, so it is reasonable to expect they wouldn’t also excel at typeface generation.',ie,M,Ue='This is not a new idea, for example in <a href="https://distill.pub/2017/aia/" rel="nofollow noopener noreferrer external" target="_blank">this article</a> they do exactly that, and approach the problem from the broader context of AI-assisted design. There are also a few Github repositories on the subject. I thought it would be fun to give it a shot myself. However, I did not try to produce typeface files per se but only typeface images, i.e. images of typeface characters, as the former would involve an awful lot of work dealing with the internal complexities of <code>otf</code> and related file formats.',oe,g,Ye='<a href="#data">Data</a>',le,k,Ze="The main problem here was to get data. Google makes their fonts publicly available, so thats around 4k examples. There are plenty of websites that offer free fonts online, but they don’t have an API of course, so there was no other way than scraping a few of them. In the end I got a bit under 130k fonts, or around 20 GB of images.",re,$,Ne="Mapping fonts to labeled images ready for model consumption was relatively easy, with an Apache Beam job on Google Cloud extracting 64-by-64 character images, and Tensorflow’s <code>Dataset</code> abstraction to consume remote files. Being free fonts, there were lots of corrupted files, corrupted characters within otherwise ok files, mislabeled characters, and fonts that looked nothing like characters.",he,H,We="I did train a classifier on the dataset and discarded all misclassified characters (around 10%); discarded images were mostly fonts or characters that were just too extravagant or simply defective, and I found that this improved the quality of generative models downstream. I also restricted the character set to uppercase to save a bit of time.",de,v,Je='<a href="#architecture">Architecture</a>',fe,L,Ke='Once plenty of clean data was available the next problem was deciding on a model architecture. I am not an expert in generative models, but thought the architecture outlined in <a href="https://arxiv.org/abs/1511.05644" rel="nofollow noopener noreferrer external" target="_blank">Adversarial Autoencoders by Goodfellow et all</a> looked good for this problem, as it enabled the model to also receive label information. I ended up doing one slight modification to this starting architecture, and the workflow looked like the following diagram (<strong>this might be hard to see with a dark background</strong>):',pe,J,I,me,P,Qe="The only difference with the paper I mentioned is that I split the encoding phase in 2: first the image is encoded by an <em>image encoder</em>, then a <em>full encoder</em> takes the encoded image features <em>and</em> the labels (this is, the one-hot-encoded charater labels) to finally produce the embedded style representation. I did this hoping that the labels help not only on the decoding phase but also on the encoding one, say, by underlining the right features given the character label, e.g. if it’s an H, curviness is probably more important to the font’s style than if its a C, which I hoped would speed up training.",ce,w,Re='<a href="#character-style-models">Character style models</a>',ue,j,Ve="The following image shows one of the model’s style components for a randomly sampled font, once the model plateaud to a MSE of around 0.020 (this is the pixelwise MSE using normalised pixels in [0,1]) by training it with minibatches of randomly sampled character images across the dataset:",ge,y,A,De,K,Xe="Transition through a straight line in feature space between two randomly generated fonts.",ve,q,et="There was a caveat though: generating all characters for a given style vector does not necessarily produce consistent image styles across the character set. I think this is because the model is only encoding the style of individual characters, as during training there is nothing that indicates any association between characters from the same font, and so, the latent style space ends up encoding styles slightly differently for different characters. To be fair, this was a relatively uncommon occurrence, but it did mean that this model wasn’t ideal for font generation.",we,x,tt='<a href="#font-style-models-a-self-supervised-approach">Font style models: a self-supervised approach</a>',ye,E,at="In order to address the caveat mentioned above, I started taking font minibatches rather than image minibatches; this restricted the training to around 70k examples of fonts that were complete (i.e., no character was lost due to corrupted data or misclassification). The trick here was to use a bit of self supervised learning to try and make the model learn the fonts’ style rather than the character style.",xe,G,nt="To do this, I shuffled the images and labels randomly when passing them to the decoder. So for example, the decoder might get the style vector from an ‘A’, but be required to reconstruct a ‘B’ instead, which should be possible to do from just the style vector and the one-hot-encoded label for ‘B’. This worked, and the styles were now consistent across characters for all style vectors, but the images were more blurry than I expected, even after the model plateaued, with a mean squared error of 0.075:",be,b,S,ze,Q,st="Transition through a straight line in feature space between two randomly generated fonts.",_e,D,it="An interesting phenomenon was that this model consistently used just 5 dimensions in the style space even when there were more than that, making the rest useless; I suspect this means that there are (broadly speaking) only as many high-level characteristics that can be generalised from a single character to entire font styles, e.g. tickness, height/width ratio and so on.",Ce,_,ot='<a href="#font-style-models-fonts-as-26-channel-images">Font style models: fonts as 26-channel images</a>',Te,z,lt="My second attempt was to take fonts as images with 26 channels where each channel was associated to a character. With this architecture, there was no need for labels anymore, as now channels acted implicitly as labels; since labels were gone, there wasn’t any need for splitting the encoding stage in 2 parts, so the whole setup reduced to the usual autoencoder architecture, plus the discriminator network on the side, simplifying things quite a bit.",Me,B,rt="This model worked better in general, achieving a lower reconstruction error and having faster training times. Since fonts are passed as multi-channel images, this is less intensive on the GPU’s memory as well, because intermediate representations are per-font and not per-image.",ke,C,F,Be,R,ht="Transition through a straight line in feature space between two randomly generated fonts.",$e,O,dt="I have to admit all results looked worse than expected at first. Then again, training high-quality generative models is not easy. Anyway, I think with a bit more data to generalise better, and with a sequence model to map images to points on the plane, (and with an expert that helps me navigate the technical aspects of font files!) it would even be possible to generate usable font files and not just images. Maybe this would be a nice bit of help for designers, to have a starting point when they set out to create a new font.",He,U,ft='This project is available on <a href="https://github.com/nestorSag/textfont-ai" rel="nofollow noopener noreferrer external" target="_blank">Github</a>, along with some pretrained decoders, and a Dash app in which to visualise style spaces.',Le,T,pt='<a href="#lessons-in-mlops">Lessons in MLOps</a>',Ie,Y,mt="This project was more than anything an excuse to get my hands dirty with MLOps practices, and I placed a lot of emphasis on this along the project. A few lessons I learned:",Pe,Z,ct="<li><p>Experiment tracking does make a difference in project organisation, and MLFlow is a great tool for this</p></li> <li><p>Configuration became the project’s center of gravity. Comprehensive YAML configuration schemas is what enabled adding complexity without chaos as a byproduct.</p></li>",je;return I=new Se({props:{src:"/textfonts-ai/images/architecture.png",alt:"architecture"}}),A=new Se({props:{src:"/textfonts-ai/images/chars.gif",alt:"chars"}}),S=new Se({props:{src:"/textfonts-ai/images/fonts.gif",alt:"fonts"}}),F=new Se({props:{src:"/textfonts-ai/images/tensor-fonts.gif",alt:"fonts"}}),{c(){h=s("p"),h.innerHTML=u,m=o(),f=s("h2"),f.innerHTML=d,p=o(),c=s("p"),c.innerHTML=Oe,ie=o(),M=s("p"),M.innerHTML=Ue,oe=o(),g=s("h2"),g.innerHTML=Ye,le=o(),k=s("p"),k.textContent=Ze,re=o(),$=s("p"),$.innerHTML=Ne,he=o(),H=s("p"),H.textContent=We,de=o(),v=s("h2"),v.innerHTML=Je,fe=o(),L=s("p"),L.innerHTML=Ke,pe=o(),J=s("p"),X(I.$$.fragment),me=o(),P=s("p"),P.innerHTML=Qe,ce=o(),w=s("h3"),w.innerHTML=Re,ue=o(),j=s("p"),j.textContent=Ve,ge=o(),y=s("p"),X(A.$$.fragment),De=o(),K=s("em"),K.textContent=Xe,ve=o(),q=s("p"),q.textContent=et,we=o(),x=s("h3"),x.innerHTML=tt,ye=o(),E=s("p"),E.textContent=at,xe=o(),G=s("p"),G.textContent=nt,be=o(),b=s("p"),X(S.$$.fragment),ze=o(),Q=s("em"),Q.textContent=st,_e=o(),D=s("p"),D.textContent=it,Ce=o(),_=s("h3"),_.innerHTML=ot,Te=o(),z=s("p"),z.textContent=lt,Me=o(),B=s("p"),B.textContent=rt,ke=o(),C=s("p"),X(F.$$.fragment),Be=o(),R=s("em"),R.textContent=ht,$e=o(),O=s("p"),O.textContent=dt,He=o(),U=s("p"),U.innerHTML=ft,Le=o(),T=s("h2"),T.innerHTML=pt,Ie=o(),Y=s("p"),Y.textContent=mt,Pe=o(),Z=s("ul"),Z.innerHTML=ct,this.h()},l(e){h=i(e,"P",{"data-svelte-h":!0}),r(h)!=="svelte-nendkn"&&(h.innerHTML=u),m=l(e),f=i(e,"H2",{id:!0,"data-svelte-h":!0}),r(f)!=="svelte-1qh4qj0"&&(f.innerHTML=d),p=l(e),c=i(e,"P",{"data-svelte-h":!0}),r(c)!=="svelte-1vjrv74"&&(c.innerHTML=Oe),ie=l(e),M=i(e,"P",{"data-svelte-h":!0}),r(M)!=="svelte-1yz8p5w"&&(M.innerHTML=Ue),oe=l(e),g=i(e,"H2",{id:!0,"data-svelte-h":!0}),r(g)!=="svelte-14ht7vg"&&(g.innerHTML=Ye),le=l(e),k=i(e,"P",{"data-svelte-h":!0}),r(k)!=="svelte-wp8n64"&&(k.textContent=Ze),re=l(e),$=i(e,"P",{"data-svelte-h":!0}),r($)!=="svelte-1yxkc6i"&&($.innerHTML=Ne),he=l(e),H=i(e,"P",{"data-svelte-h":!0}),r(H)!=="svelte-1i8z3r2"&&(H.textContent=We),de=l(e),v=i(e,"H2",{id:!0,"data-svelte-h":!0}),r(v)!=="svelte-m3c4bv"&&(v.innerHTML=Je),fe=l(e),L=i(e,"P",{"data-svelte-h":!0}),r(L)!=="svelte-82gawe"&&(L.innerHTML=Ke),pe=l(e),J=i(e,"P",{});var a=Ge(J);ee(I.$$.fragment,a),a.forEach(t),me=l(e),P=i(e,"P",{"data-svelte-h":!0}),r(P)!=="svelte-1j8344u"&&(P.innerHTML=Qe),ce=l(e),w=i(e,"H3",{id:!0,"data-svelte-h":!0}),r(w)!=="svelte-1mmysz2"&&(w.innerHTML=Re),ue=l(e),j=i(e,"P",{"data-svelte-h":!0}),r(j)!=="svelte-1sw7vkl"&&(j.textContent=Ve),ge=l(e),y=i(e,"P",{});var Ae=Ge(y);ee(A.$$.fragment,Ae),De=l(Ae),K=i(Ae,"EM",{"data-svelte-h":!0}),r(K)!=="svelte-qis0vt"&&(K.textContent=Xe),Ae.forEach(t),ve=l(e),q=i(e,"P",{"data-svelte-h":!0}),r(q)!=="svelte-178cxlz"&&(q.textContent=et),we=l(e),x=i(e,"H3",{id:!0,"data-svelte-h":!0}),r(x)!=="svelte-1b7m0xe"&&(x.innerHTML=tt),ye=l(e),E=i(e,"P",{"data-svelte-h":!0}),r(E)!=="svelte-18m7k43"&&(E.textContent=at),xe=l(e),G=i(e,"P",{"data-svelte-h":!0}),r(G)!=="svelte-v0ns15"&&(G.textContent=nt),be=l(e),b=i(e,"P",{});var qe=Ge(b);ee(S.$$.fragment,qe),ze=l(qe),Q=i(qe,"EM",{"data-svelte-h":!0}),r(Q)!=="svelte-qis0vt"&&(Q.textContent=st),qe.forEach(t),_e=l(e),D=i(e,"P",{"data-svelte-h":!0}),r(D)!=="svelte-1nbpcj9"&&(D.textContent=it),Ce=l(e),_=i(e,"H3",{id:!0,"data-svelte-h":!0}),r(_)!=="svelte-mhfqlc"&&(_.innerHTML=ot),Te=l(e),z=i(e,"P",{"data-svelte-h":!0}),r(z)!=="svelte-28uau8"&&(z.textContent=lt),Me=l(e),B=i(e,"P",{"data-svelte-h":!0}),r(B)!=="svelte-tsszeu"&&(B.textContent=rt),ke=l(e),C=i(e,"P",{});var Ee=Ge(C);ee(F.$$.fragment,Ee),Be=l(Ee),R=i(Ee,"EM",{"data-svelte-h":!0}),r(R)!=="svelte-qis0vt"&&(R.textContent=ht),Ee.forEach(t),$e=l(e),O=i(e,"P",{"data-svelte-h":!0}),r(O)!=="svelte-1mte2cv"&&(O.textContent=dt),He=l(e),U=i(e,"P",{"data-svelte-h":!0}),r(U)!=="svelte-1m84kyc"&&(U.innerHTML=ft),Le=l(e),T=i(e,"H2",{id:!0,"data-svelte-h":!0}),r(T)!=="svelte-r9vgwr"&&(T.innerHTML=pt),Ie=l(e),Y=i(e,"P",{"data-svelte-h":!0}),r(Y)!=="svelte-rhpt24"&&(Y.textContent=mt),Pe=l(e),Z=i(e,"UL",{"data-svelte-h":!0}),r(Z)!=="svelte-1s7x2od"&&(Z.innerHTML=ct),this.h()},h(){N(f,"id","motivation"),N(g,"id","data"),N(v,"id","architecture"),N(w,"id","character-style-models"),N(x,"id","font-style-models-a-self-supervised-approach"),N(_,"id","font-style-models-fonts-as-26-channel-images"),N(T,"id","lessons-in-mlops")},m(e,a){n(e,h,a),n(e,m,a),n(e,f,a),n(e,p,a),n(e,c,a),n(e,ie,a),n(e,M,a),n(e,oe,a),n(e,g,a),n(e,le,a),n(e,k,a),n(e,re,a),n(e,$,a),n(e,he,a),n(e,H,a),n(e,de,a),n(e,v,a),n(e,fe,a),n(e,L,a),n(e,pe,a),n(e,J,a),te(I,J,null),n(e,me,a),n(e,P,a),n(e,ce,a),n(e,w,a),n(e,ue,a),n(e,j,a),n(e,ge,a),n(e,y,a),te(A,y,null),V(y,De),V(y,K),n(e,ve,a),n(e,q,a),n(e,we,a),n(e,x,a),n(e,ye,a),n(e,E,a),n(e,xe,a),n(e,G,a),n(e,be,a),n(e,b,a),te(S,b,null),V(b,ze),V(b,Q),n(e,_e,a),n(e,D,a),n(e,Ce,a),n(e,_,a),n(e,Te,a),n(e,z,a),n(e,Me,a),n(e,B,a),n(e,ke,a),n(e,C,a),te(F,C,null),V(C,Be),V(C,R),n(e,$e,a),n(e,O,a),n(e,He,a),n(e,U,a),n(e,Le,a),n(e,T,a),n(e,Ie,a),n(e,Y,a),n(e,Pe,a),n(e,Z,a),je=!0},p:yt,i(e){je||(ae(I.$$.fragment,e),ae(A.$$.fragment,e),ae(S.$$.fragment,e),ae(F.$$.fragment,e),je=!0)},o(e){ne(I.$$.fragment,e),ne(A.$$.fragment,e),ne(S.$$.fragment,e),ne(F.$$.fragment,e),je=!1},d(e){e&&(t(h),t(m),t(f),t(p),t(c),t(ie),t(M),t(oe),t(g),t(le),t(k),t(re),t($),t(he),t(H),t(de),t(v),t(fe),t(L),t(pe),t(J),t(me),t(P),t(ce),t(w),t(ue),t(j),t(ge),t(y),t(ve),t(q),t(we),t(x),t(ye),t(E),t(xe),t(G),t(be),t(b),t(_e),t(D),t(Ce),t(_),t(Te),t(z),t(Me),t(B),t(ke),t(C),t($e),t(O),t(He),t(U),t(Le),t(T),t(Ie),t(Y),t(Pe),t(Z)),se(I),se(A),se(S),se(F)}}}function Mt(W){let h,u;const m=[W[0],vt];let f={$$slots:{default:[Tt]},$$scope:{ctx:W}};for(let d=0;d<m.length;d+=1)f=Fe(f,m[d]);return h=new _t({props:f}),{c(){X(h.$$.fragment)},l(d){ee(h.$$.fragment,d)},m(d,p){te(h,d,p),u=!0},p(d,[p]){const c=p&1?Ct(m,[p&1&&gt(d[0]),p&0&&gt(vt)]):{};p&2&&(c.$$scope={dirty:p,ctx:d}),h.$set(c)},i(d){u||(ae(h.$$.fragment,d),u=!0)},o(d){ne(h.$$.fragment,d),u=!1},d(d){se(h,d)}}}const vt={title:"Training Generative Models for Typefaces",tags:["computer vision","generative models"],image_caption:"Styles across one of the dimensions in latent space",image:"/textfonts-ai/images/tensor-fonts.gif",created:"2021-07-31T00:00:00.000Z",updated:"2024-01-21T20:10:06.855Z",images:[],slug:"/textfonts-ai/+page.md",path:"/textfonts-ai",toc:[{depth:2,title:"Motivation",slug:"motivation"},{depth:2,title:"Data",slug:"data"},{depth:2,title:"Architecture",slug:"architecture"},{depth:3,title:"Character style models",slug:"character-style-models"},{depth:3,title:"Font style models: a self-supervised approach",slug:"font-style-models-a-self-supervised-approach"},{depth:3,title:"Font style models: fonts as 26-channel images",slug:"font-style-models-fonts-as-26-channel-images"},{depth:2,title:"Lessons in MLOps",slug:"lessons-in-mlops"}]};function kt(W,h,u){return W.$$set=m=>{u(0,h=Fe(Fe({},h),ut(m)))},h=ut(h),[h]}class Pt extends xt{constructor(h){super(),bt(this,h,kt,Mt,wt,{})}}export{Pt as component};
